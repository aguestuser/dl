# Overview

`dl` is a program that lets you download big files fast(er** by parallelizing downloads across many threads and cores.

**Table Of Contents**
* [Getting Started](#getting-started)
  * [Sysem Depencencies](#dependencies)
  * [Building dl](#build)
  * [Running dl](#run)
  * [Developing](#develop)
* [Application Design](#design)
* [TODO](#todo)

# Getting Started <a name="geting-started"></a>

## System Dependencies <a name="dependencies"></a>

You will need `rust` and its build tool, `cargo`, installed to run `dl`. The fastest/simplest way to install them is by running this in a shell:

``` shell
curl https://sh.rustup.rs -sSf | sh
source $HOME/.cargo/env
```

If `curl`-ing bash scripts from the internet into your shell makes you nervous, you can try one of the options [here](https://forge.rust-lang.org/other-installation-methods.html#other-ways-to-install-rustup).

Whatever method you chose, you can check to make sure everything worked with:

``` shell
rustc --version
cargo --version
```

If you would like to update all your packages (likely unnecessary), you can run:

``` shell
rustup update
```

## Building dl <a name="build"></a>

Yay! Now that you have `rust` now, you can build `dl` with:

``` shell
cd <path/to/this/repo>
cargo build --release
```

To make it easier to run, let's put the build artifact on our PATH:

``` shell
sudo ln -s ${PWD}/target/release/dl /usr/bin/dl
```

(If you don't want to do that, that's okay! We can use a slightly longer command later!)

## Running dl <a name="run"></a>

Okay! Time for the main event! You can run dl with:

``` shell
dl <url_to_download_from> <path_to_save_file_to>
```

(Filling in <url_to_download_from> and <path_to_save_file_to> with your own values.)

For example, to download this ~20mb copy of `Designing Data-Intensive Applications` (disguised as my resume on RC's S3 bucket), you could run:

``` shell
dl https://recurse-uploads-production.s3.amazonaws.com/dc12e4d0-3c82-45b8-9cb7-6c64a8f50cfb/austin_guest_resume.pdf data/book.pdf
```

If you didn't symlink the build artifact above, you could run:

``` shell
./target/release/dl <url> <path>
```

You could also just run `dl "foo bar" "123"`, but that won't be terribly interesting! ;**

**NOTE ABOUT VERY LARGE FILES:**

`dl` does not behave particularly well for files on the order of several hundred megabytes.

For test files around 300mb, I have observed:

1. `too many open file` errors from the OS (caused by a flood of request causing too many sockets to remain open at the same time)
2. `connection reset by peer` errors from the server (presumably caused by our unthrottled traffic looking like an attack)

(1) can be (sloppily!) worked around by increasing the limit on open files for your shell session to the maximum allowed with:

``` shell
ulimit -n $(ulimit -Hn)
```

Fixing (2) in a robust way would require:

(a) tracking the status of each request and retrying on failure until all have completed successfully
(b) throttling the number of requests in flight at any moment (which would have the happy effect of more elegantly mitigating (1))

Sadly, I did not have time to do either for this challenge, but it's what I'd like to do if I had more time.


## Developing dl ## Running dl <a name="develop"></a>

In the above we used production builds because they are faster, and this is a **challenge!** However, if we wanted to hack on the project to change it, we'd want faster build times than come with the release flag. We'd build with a simple:

``` shell
cargo build
```

And run the tests (with frequency, right?) with:

``` shell
cargo test
```

If you want to check out the (nifty, autogenerated!) docs, you can always run:

``` shell
cargo doc --open
```

# Application Design <a name="design"></a>

If you want to find your way around the code, starting in `main`, then looking at `lib` and to the modules that it calls is a good way in.

The main action happens in `lib::run`, which is structured as a pipeline of futures, each of which consumes and produces a struct that encapsulates the current state of the program.

More specifically, program flow is modeled a transition between the following structs:

``` shell
Config -> MetadataDownloader -> FileDownloader -> HashChecker
```

This encoding of application state as transitions between structs is a rust convention which, in addition to trying to provide clarity as to what phase of execution the app is in, does some memory management work for us under the hood.

As should be evident from the above, the flow of the program is to:

- accept configs from the user
- make a `HEAD` request to the user-supplied url to:
  - see if it supports range requests, and if so:
  - retrieve the length of the file and (if it exists) its etag
- download the file in several parallel chunks
- take the hash of the downloaded file to see if it matches the advertised etag

Most of the heavy lifting comes in `dl::file::FileDownloader::fetch`. This function:

- creates a blank placeholder file into which chunks will be written as soon as they come off the wire
- calculates optimally-sized chunks in which to download the file (generating larger pieces for larger files), then generates a stream of chunk-sized offsets to use in range requests)
- consumes the stream of offsets and fires of a stream of parallel range requests for the corresponding chunk
- transforms the stream of responses into a stream of buffered bytes, which it writes to the placeholder file after seeking to the correct offset (note: all writes are performed in parallel)
- collects this stream of futures into a single future that resolves successfully if all requests resovle successfully and with failure if any requests fail (yes: we could be less brittle than that in future iterations!)

A brief note on types:

Each call to `download_piece` returns a value that we represent as a `Future<u64, DlError>`. The `u64` represents the offset of the downloaded chunk-- in case we wanted to track the status of each request for retries, which we don't currently do (very brittle, I know!).

The`DlError` is a custom error class that serves as an enumerable set of all possible errors we might see in the program's execution. Constructing this type involves a lot of boilerplate. (See `dl::errors`). BUT... having this type around turns out to do a lot of work. For starters, it's **necessary*** to make the "types line up" for all of our variously-typed futures to return an error component of the same type, which allows us to use (monadic) `map`, `map_err`, and `and_then` combinators to manage control flow.

It also makes error-handling in the main loop a breeze, since we have a strong guarantee that our main loop will only ever produce (future) errors of a certain type and that execution will short circuit as soon as one is encountered. Thus, we can just call our main function and `map_err` over its results to print the correct error at the correct time without worrying too much about it. If we wanted to bucket errors into smaller groupings for a larger calling function, that would also be easy. I found all this business with errors annoying at first, but in the end I liked it!

if you see a `Box<Future>` and wonder what's going on: its main purpose is to make sure the compiler has enough information about what kind of future we're returning when we compose Futures of slightly varying types.

# Profiling <a name="profiling"></a>

My main goal in building this project was to build something that was fast, but not necessarily fault-tolerant. (And to learn about async programming in rust!)

On this score, I think I did okay. To make sure this was true, I performed some elementary profiling to compare the performance of my parallel solution with a straight-up `GET` request.

The benchmarks from these profiling experiments live with the repo. You can view them in the browser with (for example):

``` shell
cd path/to/this/repo
firefox target/criterion/report/index.html
```

The upshot is that the parallel solution performs increasingly better than its sequential counterpart the larger the files it is downloading.

For small files (on the order of 50KB) both solutions perform roughly the same: downloading files in ~400ms.

For medium sized files (on the order of 50MB), the parallel solution outperforms its sequential counterpart by roughly 4x (15s vs 90s).

For large files (on the order of 500MB), I don't have any data available because my solution breaks at that scale.

Given time, I would like to further develop the app to see whether the gains from parallelization scale quadratically, exponentially, etc. and whether they plateau at some point.

# TODO <a name="todo"></a>

As noted above, my solution has two major flaws:

(1) It does not retry failed chunk requests and suffers halting errors when they happen
(2) It does not throttle requests and suffers halting errors when the server resets the connection as a result

Additionally:

(3) It relies on discovering the file of the size in advance (to calculate chunk sizes) but has no fallback measures in place for discovering that information in the case that the target server does not supply in in the response to a `HEAD` request.

If I had more time to work on this project, I'd want to solve the above problems in roughly that order. Here are some preliminary thoughts on how I'd do so:

## 1. Handling Failed Requests

To handle failed requests, first we'd want to track the status of all requests. We'd need to store this data in some sort of thread-safe data structure that a bunch of parallel threads could read fro mand write to in parallel without producing contention or draining resources in acquiring locks, etc. My data structure of choice would be some sort of concurrent hash map (which has a series of distribtued mutexes on entries the map, rather than one mutex on the whole map). It looks like rust has a decent implementation in [chashmap](https://docs.rs/chashmap/2.2.2/chashmap/).

When `FileDownloader#fetch` was called, I'd pass it an empty `chashmap`. As requests or writes fail and/or succeed I'd write record that fact to an entry in the `chashmap`. As a first approximation, let's say the keys of the map would be the number of the offset, and the values would be an enum with possible values `Success`, `FailedRequest`, `FailedWrite`. If either a request or a write failed, one of the futures in the stream being collected into one future would fail, meaning that the entire future would fail. I'd `map_err` over this future and recursively call `fetch` again, feeding it the (now not-empty) version of the hash-map.

On subsequent calls, `fetch` should omit offset whose keys contain `Success` values from the stream of offsets it generates for range requests. (Which means that on initial call, it should also consult the empty map to help it construct a full set of offsets.)

To add in further resilience, we could consider the case that execution might terminate in the middle of downloading or writing. To recover from this, we could serialize our `chashmap` on some sane interval and write it to disk, and always make an attempt to build the initial `chashmap` that we pass into `fetch` by deserializing whatever we do (or don't) have stored to disk.

## 2. Throttling Requests

I tried to do this was frankly surprised at how difficult it was.

At first pass [futures::stream::buffered](https://docs.rs/futures/0.1.27/futures/stream/trait.Stream.html#method.buffered) seemed to be the way to go. The idea is that you'd pass a concurrency limit as the value of `amt` and only ever have that many futures in flight at the same time. However, I tried but couldn't figure out how to get the types to line up to get this to both (1) consume the entire stream without terminating early (it appeared to only consume the `amt` number, then stop? like `stream::take` does?), (2) return a future of the right type whenever any unit of processing was complete. It's a bit of a head scratcher, and I'm honestly a bit tired of fighting the rust compiler after several days straight at it. But if I had to take a guess at where to start, this would be it.

Some interesting prior art around the `too many files open` bug that I've been poking around include:

* [issue on hyper](https://github.com/hyperium/hyper/issues/1422)
* [issue on reqwest -- another tokio-based http lib](https://github.com/seanmonstar/reqwest/issues/386)
* [handrolled channel-based fix](https://github.com/sybblow/rust-concurrent-http-request/blob/master/src/main.rs) based on discussion in [this reddit thread](https://www.reddit.com/r/rust/comments/68bd6h/tokio_hyper_and_many_client_requests/)

In any case, I take some comfort in knowing that I seem to be in somewhat decent company at having a hard time cracking this problem.

## 3. Fallback file-size discovery

I left a seam for introducing a functional take on a "strategy pattern" solution to this problem in `MetadataDownloader::fetch`. The idea would be that we would call a pipeline of composed functions to try to discover the file size. If any of them find the answer, we return early. If all of them fail, we return an error, causing the caller's execution to short circuit.

I can think of 3 strategies to try (in increasing order of cleverness):

1. Ask for the length in a `HEAD` request (what we currently do). If that fails...
2. Issue a range request for bytes 0-1 of the file. Inspect the `Content-Lenghth` and/or `Content-Range` headers and extract the size of the file from the header values, if present. If that fails...
3. Perform a binary-search like search for the last byte, when you find it, return its position as the file size

As (3) is the only non-trivial solution, its implementation bears some discussion. I'd imagine something like the following:

- write a recursive function that request a one byte chunk of the file at index 2^N, with N starting at 0. if you get data in response increase N by 1 until you don't see a response.
- remeber the first `N` at which you didn't get a response and the last `N` at which you did. call the former `upper` and the latter `lower`
- write a recursive function that implements binary search between `upper` and `lower`:
  - query the server for data at an index halfway between `upper` and `lower`
  - if you find data at an index, set `lower` to that index and recurse
  - if you don't find data at an index, set `upper` to that index and recurse
  - terminate the recursion when `lower` is one less than `upper`, and return `lower`

This should find the size of the file in roughly `O(log N)`, where `N` is the size of the file in bytes.
